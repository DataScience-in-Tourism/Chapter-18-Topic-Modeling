{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXHLDxJdRzBi"
   },
   "source": [
    "# Chapter 18: Topic Modelling (BERTopic)\n",
    "### Roman Egger\n",
    "##### <italic>Salzburg University of Applied Sciences - Department: Innovation and Management in Tourism</italic>\n",
    "\n",
    "---\n",
    "\n",
    "In this Jupyter Notebook, we will do a complete topic modelling walkthrough with a dataset from airbnb using BERTopic\n",
    "\n",
    "The dataset we will use to extract topics from was crawled by the author and contains 2890 descriptions of airbnb-Experiences from the following European cities: Amsterdam, Athens, Berlin, Brussels, Copenhagen, Helsinki, London, Madrid, Oslo, Paris, Prague, Rome, Stockholm, Viwenna and Warsaw. \n",
    "Open the dataset (csv) [here](data/Airbnb_total.csv)\n",
    "\n",
    "<img src=\"data/paris.jpg\">\n",
    "\n",
    "---\n",
    "[See such an airbnb example](https://www.airbnb.com/experiences/356769?currentTab=experience_tab&federatedSearchId=9297b301-0091-433b-899d-0bcda11332a9&searchId=&sectionId=704c8a0a-1f93-4442-b6b5-44b52d817c5b&source=p2)\n",
    "\n",
    "\n",
    "---\n",
    "### We will go through the following steps:\n",
    "* #### Data Preperation & Preprocessing\n",
    "\n",
    "<hr>\n",
    "\n",
    "Aknowledgement:<br>\n",
    "This notebook is based on the [BERTopic Project by Maarten Grootendorst](https://maartengr.github.io/BERTopic/)\n",
    "<br>\n",
    "[Related Medium-Post](https://towardsdatascience.com/interactive-topic-modeling-with-bertopic-1ea55e7d73d8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNa-KtKDRnus",
    "outputId": "263dfc86-4e60-45b8-a716-bb50ed04a44f"
   },
   "outputs": [],
   "source": [
    "#!pip install bertopic[visualization]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3VGFZ1USMTu"
   },
   "source": [
    "# **Prepare data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJij3WP6SEQD",
    "outputId": "6b4d3f7b-9f7f-426f-dea8-ab1e5083eb94"
   },
   "outputs": [],
   "source": [
    "# Let´s import the modules needed and load the Airbnb dataset.\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import os \n",
    "import umap\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from  plotting_utils import *\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib as mpl\n",
    "\n",
    "docs = pd.read_csv(r'./data/Airbnb_total.csv', sep=\";\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=docs.dropna(subset=[\"Todo\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs.drop(columns=['ID', 'ID.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case\n",
    "docs['prep'] = docs['Todo'].str.lower()\n",
    "# Remove square brackets and text in square brackets\n",
    "regex = r\"\\[.*?\\]\"\n",
    "docs['prep'] = docs['prep'].str.replace(regex, '')\n",
    "# Remove punctuation\n",
    "regex = r'[^\\w\\s]'\n",
    "docs['prep'] = docs['prep'].str.replace(regex,'')\n",
    "# Remove words containing numbers\n",
    "regex = r\"([A-Za-z]+[\\d@]+[\\w@]*|[\\d@]+[A-Za-z]+[\\w@]*)\"\n",
    "docs['prep'] = docs['prep'].str.replace(regex, '')\n",
    "# Remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "docs['prep'] = docs['prep'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop) and len(word) > 2]))\n",
    "\n",
    "# Tokenize sentences\n",
    "def lemmatizer(text):        \n",
    "    sent = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent)\n",
    "docs[\"lemmatized\"] =  docs.apply(lambda x: lemmatizer(x['prep']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Todo</th>\n",
       "      <th>prep</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>First of all we want to thank you all for choo...</td>\n",
       "      <td>first want thank choosing experience proud ann...</td>\n",
       "      <td>first want thank choose experience proud annou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>We will have an exclusive Morning boat tour th...</td>\n",
       "      <td>exclusive morning boat tour amsterdam canals c...</td>\n",
       "      <td>exclusive morning boat tour amsterdam canals c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>*PLEASE NOTE THIS IS A FREE TOUR CONCEPT*\\n(1 ...</td>\n",
       "      <td>please note free tour concept euro ensure spot...</td>\n",
       "      <td>please note free tour concept euro ensure spot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>For our Winter Warmer Premium Cruise, we invit...</td>\n",
       "      <td>winter warmer premium cruise invite hour allin...</td>\n",
       "      <td>winter warm premium cruise invite hour allincl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>I am a social media photographer, and I would ...</td>\n",
       "      <td>social media photographer would love take tour...</td>\n",
       "      <td>social medium photographer would love take tou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City                                               Todo  \\\n",
       "0  Amsterdam  First of all we want to thank you all for choo...   \n",
       "1  Amsterdam  We will have an exclusive Morning boat tour th...   \n",
       "2  Amsterdam  *PLEASE NOTE THIS IS A FREE TOUR CONCEPT*\\n(1 ...   \n",
       "3  Amsterdam  For our Winter Warmer Premium Cruise, we invit...   \n",
       "4  Amsterdam  I am a social media photographer, and I would ...   \n",
       "\n",
       "                                                prep  \\\n",
       "0  first want thank choosing experience proud ann...   \n",
       "1  exclusive morning boat tour amsterdam canals c...   \n",
       "2  please note free tour concept euro ensure spot...   \n",
       "3  winter warmer premium cruise invite hour allin...   \n",
       "4  social media photographer would love take tour...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  first want thank choose experience proud annou...  \n",
       "1  exclusive morning boat tour amsterdam canals c...  \n",
       "2  please note free tour concept euro ensure spot...  \n",
       "3  winter warm premium cruise invite hour allincl...  \n",
       "4  social medium photographer would love take tou...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBcNmZJzSTY8"
   },
   "source": [
    "# **Create Topics**\n",
    "English is the default setting. However BERTopic supports also multilingual corpra with more than 50 languages. In this case change \"english\" to \"multilingual\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'low_memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2fed4bcd0b54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lemmatized\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, language, top_n_words, n_gram_range, min_topic_size, nr_topics, low_memory, calculate_probabilities, embedding_model, umap_model, hdbscan_model, vectorizer_model, verbose)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;31m# UMAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         self.umap_model = umap_model or umap.UMAP(n_neighbors=15,\n\u001b[0m\u001b[0;32m    158\u001b[0m                                                   \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                                                   \u001b[0mmin_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'low_memory'"
     ]
    }
   ],
   "source": [
    "model = BERTopic(language=\"english\")\n",
    "topics, probs = model.fit_transform(docs[\"lemmatized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to see, which topics are supported\n",
    "#from bertopic import languages\n",
    "#print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5d090fe5e25f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ua80usww-rj"
   },
   "source": [
    "We can then extract most frequent topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "nNptKBzHSbyS",
    "outputId": "25855ec5-d642-4864-cc64-404df61fabc6"
   },
   "outputs": [],
   "source": [
    "model.get_topic_freq().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BtOgifV7Q-H"
   },
   "source": [
    "-1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCw_q8I7Sb03",
    "outputId": "0076f8e0-3355-409f-f3f2-6e3478c6c1ba"
   },
   "outputs": [],
   "source": [
    "model.get_topic(0)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.find_topics(\"experience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_XSeuhs7bpK"
   },
   "source": [
    "Note that the model is stocastich which means that the topics might differ across runs. \n",
    "\n",
    "For a full list of support languages, see the values below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eyImbal7lb8"
   },
   "source": [
    "# **Embedding model**\n",
    "You can select any model from `sentence-transformers` and use it instead of the preselected models by simply passing the model through  \n",
    "BERTopic with `embedding_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dR2ckNK782p"
   },
   "outputs": [],
   "source": [
    "st_model = BERTopic(embedding_model=\"xlm-r-bert-base-nli-stsb-mean-tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoMc1W-x7-b5"
   },
   "source": [
    "Click [here](https://www.sbert.net/docs/pretrained_models.html) for a list of supported sentence transformers models.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8c8LenB8Zyl"
   },
   "source": [
    "# **Visualize Topics**\n",
    "After having trained our `BERTopic` model, we can iteratively go through perhaps a hundred topic to get a good \n",
    "understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. \n",
    "Instead, we can visualize the topics that were generated in a way very similar to \n",
    "[LDAvis](https://github.com/cpsievert/LDAvis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "AQKBcla28bY0",
    "outputId": "582cefe4-a82c-4915-f2e0-3ad307d3c723",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier bitte eine Liste der Verfügbaren Städte anzeigen so dass man im Folgenden die Topics und Visualisierung für eine bestimmte Stadt anzeigen lassen kann (zb. Warschau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualize documents - use dropdown to switch between cities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data frame with topic id\n",
    "docs['BERT_Topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data frame with topic keywords\n",
    "tdict =  model.get_topics()\n",
    "docs['BERT_Topic_Keywords'] = docs['BERT_Topic'].apply(lambda x: [i[0] for i in tdict[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document vectors (tfidf)\n",
    "docs[\"splited\"] = docs[\"Todo\"].map(lambda x: x.split())\n",
    "text_string = [' '.join(d) for d in docs['splited'].tolist()]\n",
    "n_features=10000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, ngram_range=(1,2), stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(text_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topic_string column to conform with the plotting function\n",
    "docs['topic_string'] = topics\n",
    "# UMAP embedding\n",
    "umap_embr = umap.UMAP(n_neighbors=10, metric='cosine', min_dist=0.1, init='random', random_state=42)\n",
    "embedding = umap_embr.fit_transform(tfidf.toarray())\n",
    "embedding = pd.DataFrame(embedding, columns=['x','y'])\n",
    "docs = pd.concat([docs, embedding],1 )\n",
    "\n",
    "# Visalize with custom function\n",
    "plot_main(docs, num_topics=np.unique(topics), save_name='results/BERT_topics.html', model= 'BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Get topic most similar to search tearm-city name \n",
    "# countries = docs.City.unique()\n",
    "# country_topics = {country: model.find_topics(country)[0][0] for country in countries}\n",
    "# country_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot only these topics that refer to cities\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import umap\n",
    "# import numpy as np\n",
    "\n",
    "# topic_list = sorted(list(country_topics.values()))\n",
    "# topic_list = np.unique(topic_list)\n",
    "# frequencies = [model.topic_sizes[topic] for topic in topic_list]\n",
    "# words = [\" | \".join([word[0] for word in model.get_topic(topic)[:5]]) for topic in topic_list]\n",
    "\n",
    "# # # Embed c-TF-IDF into 2D\n",
    "# embeddings = MinMaxScaler().fit_transform(model.c_tf_idf.toarray())\n",
    "# embeddings = umap.UMAP(n_neighbors=2, n_components=2, metric='hellinger').fit_transform(embeddings)\n",
    "\n",
    "# #Filter embeddings\n",
    "# #topic_list_unique = np.unique(topic_list)\n",
    "# mask = [True if i in topic_list else False for i in range(len(embeddings)) ]\n",
    "# embeddings = embeddings[mask, :]\n",
    "\n",
    "# # Visualize \n",
    "# df = pd.DataFrame({\"x\": embeddings[:, 0], \"y\": embeddings[:, 1],\n",
    "#                    \"Topic\": topic_list, \"Words\": words, \"Size\": frequencies})\n",
    "# model._plotly_topic_visualization(df, topic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Wordclaud AU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "au = docs[docs.City=='Vienna']\n",
    "\n",
    "mpl.rcParams['figure.figsize']=(12.0,12.0)  \n",
    "mpl.rcParams['font.size']=12            \n",
    "mpl.rcParams['savefig.dpi']=100             \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stopwords,\n",
    "                          max_words=500,\n",
    "                          max_font_size=40, \n",
    "                          random_state=42\n",
    "                         ).generate(str(au['lemmatized']))\n",
    "\n",
    "print(wordcloud)\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITB7bf6q8nWQ"
   },
   "source": [
    "# **Visualize Topic Probabilities**\n",
    "\n",
    "The variable `probabilities` that is returned from `transform()` or `fit_transform()` can \n",
    "be used to understand how confident BERTopic is that certain topics can be found in a document. \n",
    "\n",
    "To visualize the distributions, we simply call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "vsd6Uce38bfX",
    "outputId": "dcb28d5a-6289-4ae5-af42-0a4c5b9efdc2"
   },
   "outputs": [],
   "source": [
    "model.visualize_distribution(probs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9antKpdC91A-"
   },
   "source": [
    "# **Topic Reduction**\n",
    "Finally, we can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, \n",
    "is that you can decide the number of topics after knowing how many are actually created. It is difficult to \n",
    "predict before training your model how many topics that are in your documents and how many will be extracted. \n",
    "Instead, we can decide afterwards how many topics seems realistic:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5m4Nd7Us-Peg"
   },
   "outputs": [],
   "source": [
    "new_topics, new_probs = model.reduce_topics(docs['lemmatized'].tolist(), topics, probs, nr_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plot after reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = docs.copy()\n",
    "# Update data frame with new topic id\n",
    "docs_new['BERT_Topic'] = new_topics\n",
    "docs_new['topic_string'] = new_topics\n",
    "# Update data frame with new topic keywords\n",
    "tdict =  model.get_topics()\n",
    "docs_new['BERT_Topic_Keywords'] = docs_new['BERT_Topic'].apply(lambda x: [i[0] for i in tdict[x]])\n",
    "\n",
    "\n",
    "# Visualize with custom function\n",
    "plot_main(docs_new, num_topics=np.unique(new_topics), save_name='results/BERT_topics_reduced.html', model= 'BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlTrmhoo-PmV"
   },
   "source": [
    "\n",
    "The reasoning for putting `docs`, `topics`, and `probs` as parameters is that these values are not saved within \n",
    "BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic \n",
    "instead of a dedicated database.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4m3UMsw-Zxk"
   },
   "source": [
    "# **Topic Representation**\n",
    "When you have trained a model and viewed the topics and the words that represent them,\n",
    "you might not be satisfied with the representation. Perhaps you forgot to remove\n",
    "stop_words or you want to try out a different n_gram_range. We can use the function `update_topics` to update \n",
    "the topic representation with new parameters for `c-TF-IDF`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWm7B-FJ-iYW"
   },
   "outputs": [],
   "source": [
    "model.update_topics(docs['lemmatized'].tolist(), topics, n_gram_range=(1, 3), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_freq().head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_distribution(probs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXYJ745O-03Z"
   },
   "source": [
    "# **Search Topics**\n",
    "After having trained our model, we can use `find_topics` to search for topics that are similar \n",
    "to an input search_term. Here, we are going to be searching for topics that closely relate the \n",
    "search term \"vehicle\". Then, we extract the most similar topic and check the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAdiVYej-2i-",
    "outputId": "29d69ab2-1e0d-4561-8cee-e43654e4479e"
   },
   "outputs": [],
   "source": [
    "similar_topics, similarity = model.find_topics(\"holiday\", top_n=5); similar_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-DaaqSA-2nH",
    "outputId": "99027647-f939-4f9d-9707-7e2f7dbb7b9a"
   },
   "outputs": [],
   "source": [
    "model.get_topic(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wekNoQNuUVoU"
   },
   "source": [
    "# **Model serialization**\n",
    "The model and its internal settings can easily be saved. Note that the documents and embeddings will not be saved. However, UMAP and HDBSCAN will be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWUF1uxiSb_a"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "#model.save(\"my_model\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_eHBI1jSb6i"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "#my_model = BERTopic.load(\"my_model\")\t"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERTopic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
